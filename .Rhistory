########################
######NA'S
#Check numbers of NA's
summary(data)
###### CORRELATIONS
# simple matrix correlation plot
plot(data[,-1])
# more fency correlation matrix plot
pairs(data[,-1],
panel = function (x, y, ...) {
points(x, y, ...)
abline(lm(y ~ x), col = "red")
}, pch = ".", cex = 1.5)
# super fency correlation
#install.packages("PerformanceAnalytics")
library("PerformanceAnalytics")
chart.Correlation(data[,-1], histogram=TRUE, pch=42)
# correlation matrix (numbers)
cor(data[,-1], use = "complete.obs")
cor(data[,-1], use = "pairwise.complete.obs")
#Check PC
data_test <- na.omit(data)
data
a <- princomp(data_test[,-1], cor=T)
summary(a, loadings=T)
# HEre is a test where I deleted few columns.
# See how PC increases/descreases while deleting less correlated columns
## TEST PCA WITHOUT FEW COLUMNS
test <- data[,-c(2,4,9,10,15)] ## optioaly delete murder_pp
test2 <- na.omit(test)
b <- princomp(test2[,-1], cor=T)
summary(b, loadings=T)
chart.Correlation(test[,-1], histogram=TRUE, pch=42)
# Create new a column continent and divide countires by continent
library(countrycode)
data$continent <- countrycode(sourcevar = data[, "country"],
origin = "country.name",
destination = "continent")
# Just rearanage columns to get continent before country
data <- data[,c(ncol(data),seq(1,ncol(data)-1,by=1))]
#The function above did not divide coutries into America North, Central, South so I did this by hand
subset(data, continent=='Americas') %>% select(country) %>% as.list()
America_North <- c("Canada","United States","Mexico")
America_South <- c("Peru","Ecuador","Chile","Bolivia","Venezuela","Paraguay","Brazil",
"Colombia",  "Guyana", "Suriname", "Uruguay", "Argentina")
#Change the values from "Americas" into more specific part of America
for (i in 1:nrow(data)){
ifelse(data$continent[i] == "Americas",
ifelse(data$country[i] %in% America_North,data$continent[i] <- 'North America',
ifelse(data$country[i] %in% America_South,data$continent[i] <- 'South America',
data$continent[i] <- 'Central America')), data$country[i])
}
#Now we can code continent as binary
data.s=scale(data)
data.s=scale(data[,c(-1,2)])
data.s=scale(data[,c(-1,-2)])
data.s=scale(data[,c(-1,-2)])
options(digits = 3)
d <- dist(data.s)
cmd <- cmdscale(data.s)
cmd
cmd <- cmdscale(data.s)
cmd <- cmdscale(d)
cmd
cmd <- cmdscale(d,eig=TRUE)
cmd
cmd <- cmdscale(d,k=5,eig=TRUE)
cmd
cmd <- cmdscale(d,k=5,eig=T)
cmd
cumsum(cmd$eig[1:5])/sum(cmd$eig[1:5])
cmd3=cmdscale(d,k=3,eig=T)
cmd3
plot(cmd3)
plot(cmdscale(dist(mydata.s)), xlab = "coordinate 1", ylab = "coordinate 2")
library("scatterplot3d")
scatterplot3d(cmdscale(dist(X), k=3))
plot(cmdscale(dist(mydata.s)), xlab = "coordinate 1", ylab = "coordinate 2")
plot(cmdscale(dist(data.s)), xlab = "coordinate 1", ylab = "coordinate 2")
library("scatterplot3d")
install.packages("scatterplot3d")
library("scatterplot3d")
scatterplot3d(cmdscale(dist(X), k=3))
scatterplot3d(cmdscale(dist(data.s), k=3))
plot(cmdscale(dist(data.s)), xlab = "coordinate 1", ylab = "coordinate 2")
plot(cmdscale(dist(data.s)), xlab = "coordinate 1", ylab = "coordinate 2")
plot(cmdscale(dist(data)), xlab = "coordinate 1", ylab = "coordinate 2")
?scatterplot3d
scatterplot3d(cmdscale(dist(data.s), k=3),tick.marks=F)
scatterplot3d(cmdscale(dist(data.s), k=3),ced.symbols=par('cex'))
scatterplot3d(cmdscale(dist(data.s), k=3),cex.symbols =par('cex'))
scatterplot3d(cmdscale(dist(data.s), k=3),pch='.')
scatterplot3d(cmdscale(dist(data.s), k=3),pch='.',angle=45)
scatterplot3d(cmdscale(dist(data.s), k=3),pch='.',angle=15)
scatterplot3d(cmdscale(dist(data.s), k=3),pch='.',angle=30)
scatterplot3d(cmdscale(dist(data.s), k=3),pch='.',angle=75)
scatterplot3d(cmdscale(dist(data.s), k=3),pch='.',angle=185)
scatterplot3d(cmdscale(dist(data.s), k=3),pch='.',angle=35)
scatterplot3d(cmdscale(dist(data.s), k=3),pch='text',angle=35)
scatterplot3d(cmdscale(dist(data.s), k=3),pch=data[,2],angle=35)
scatterplot3d(cmdscale(dist(data.s), k=3),pch=data[,2],angle=35,type=h)
scatterplot3d(cmdscale(dist(data.s), k=3),pch=data[,2],angle=35,type='h')
scatterplot3d(cmdscale(dist(data.s), k=3),pch=data[,2],angle=35,type='l')
scatterplot3d(cmdscale(dist(data.s), k=3),pch=data[,2],angle=35,type='h')
scatterplot3d(cmdscale(dist(data.s), k=3),pch=data[,2],angle=35,type='h',highlight.3d)
scatterplot3d(cmdscale(dist(data.s), k=3),pch=16,angle=35,type='h',highlight.3d)
scatterplot3d(cmdscale(dist(data.s), k=3),pch=16,angle=35,type='h')
scatterplot3d(cmdscale(dist(data.s), k=3),pch=5,angle=35,type='h')
scatterplot3d(cmdscale(dist(data.s), k=3),pch=5,angle=45,type='h')
scatterplot3d(cmdscale(dist(data.s), k=3),pch=5,angle=180,type='h')
scatterplot3d(cmdscale(dist(data.s), k=3),pch=5,angle=115,type='h')
scatterplot3d(cmdscale(dist(data.s), k=3),pch=5,angle=0,type='h')
scatterplot3d(cmdscale(dist(data.s), k=3),pch=5,angle=25,type='h')
cumsum(cmd$eig[1:5])/sum(cmd$eig[1:5])
data
plot(cmdscale(dist(data)), xlab = "coordinate 1", ylab = "coordinate 2")
plot(cmdscale(dist(data.s)), xlab = "coordinate 1", ylab = "coordinate 2")
?plot
plot(cmdscale(d, xlab = "coordinate 1", ylab = "coordinate 2")
plot(cmdscale(d, xlab = "coordinate 1", ylab = "coordinate 2"))
plot(cmdscale(d, xlab = "coordinate 1", ylab = "coordinate 2"))
plot(cmdscale(dist(data.s), xlab = "coordinate 1", ylab = "coordinate 2"))
plot(cmdscale(dist(data.s), xlab = "coordinate 1", ylab = "coordinate 2"))
dist(data.s))
plot(cmdscale(dist(data.s)), xlab = "coordinate 1", ylab = "coordinate 2")
cumsum(cmd$eig[1:5])/sum(cmd$eig[1:5]) #80% of variance explained with 3
for(q in 1:ncol(data)){
data[is.na(data[,q]),q]=median(data[,q],na.rm=TRUE)
}
data.s=scale(data[,c(-1,-2)])
d <- dist(data.s)
cmd <- cmdscale(d,k=5,eig=T)
cumsum(cmd$eig[1:5])/sum(cmd$eig[1:5]) #80% of variance explained with 3
cmd3=cmdscale(d,k=3,eig=T)
cmd3
plot(cmdscale(dist(data.s)), xlab = "coordinate 1", ylab = "coordinate 2")
text(colnames(data[,2]))
text(cmd$points[,1:2],labels = colnames(data[,2],cex = 0.6))
text(cmd$points[,1:2],labels = colnames(data[,2])
text(cmd$points[,1:2],labels = colnames(data[,2]))
plot(cmdscale(dist(data.s)), xlab = "coordinate 1", ylab = "coordinate 2")
text(cmd$points[,1:2],labels = colnames(data[,2]))
text(cmd$points[,1:2],labels = rownames(cmd))
cumsum(cmd$eig[1:5])/sum(cmd$eig[1:5]) #80% of variance explained with 3
cmd3=cmdscale(d,k=3,eig=T)
cmd3
plot(cmdscale(dist(data.s)), xlab = "coordinate 1", ylab = "coordinate 2")
text(cmd$points[,1:2],labels = rownames(cmd))
?group_by
group_by(data,'continent')
data.g=group_by(data,'continent')
data.g
data.g=tibble(data[,1])
data.g
group_rows(data)
d=group_rows(data)
d
?group_data
aggregate(Score~Rptname, transform(data, Rptname=substr(Rptname, 1,15)), sum)
aggregate(countrycode~continent, transform(data, continent=substr(continent, 1,15)), sum)
aggregate(countrycode~continent, transform(data, continent=substr(continent, 1,15)))
group_by(.data)
by_cont <- data %>% group_by(continent)
by_cont
data.sg=scale(by_cont)
print(by_cont)
data.g <- data %>% group_by(continent)
data.g
data.g <- (data %>% group_by(continent),mean)
aggregate(countrycode ~ continent, data, mean)
round(aggregate(data[,3:17], list(data$continent), mean),3)
aggregate(data[,3:17], list(data$continent), mean)
options(digits=3)
aggregate(data[,3:17], list(data$continent), mean)
options(digits=3)
aggregate(data[,3:17], list(data$continent), mean)
options(digits=3)
data.g=aggregate(data[,3:17], list(data$continent), mean)
cmd2 = cmdscale(d2,k=5,eig=T)
datag.s = scale(data.g[,c(-1,-1)])
d2 = dist(datag.s)
cmd2 = cmdscale(d2,k=5,eig=T)
cumsum(cmd2$eig[1:5])/sum(cmd2$eig[1:5])
cumsum(cmd2$eig[1:5])/sum(cmd2$eig[1:5])
datag.s = scale(data.g[,c(-1,-2)])
d2 = dist(datag.s)
cmd2 = cmdscale(d2,k=3,eig=T)
cumsum(cmd2$eig[1:5])/sum(cmd2$eig[1:5])
plot(cmdscale(dist(datag.s)), xlab = "coordinate 1", ylab = "coordinate 2")
scatterplot3d(cmdscale(dist(data), k=3))
scatterplot3d(cmdscale(dist(data.g), k=3))
datag=aggregate(data[,3:17], list(data$continent), mean)
datag.s = scale(datag[,c(-1,-2)])
d2 = dist(datag.s)
cmd2 = cmdscale(d2,k=3,eig=T)
cumsum(cmd2$eig[1:5])/sum(cmd2$eig[1:5]) # to check the first 5
plot(cmdscale(dist(datag.s)), xlab = "coordinate 1", ylab = "coordinate 2")
scatterplot3d(cmdscale(dist(datag), k=3))
?legend
plot(mbc, what='uncertainty', dimens= c(6,8))
euro <- read.csv("http://tiny.cc/isqs6350_euroemp", header=TRUE, row.names=1)
head(euro)
# Remove outliers (removing Albania and Gibraltar),
euro.c <- euro[-c(19,28), ]
# and focus on numerical variables (removing the first column)
mydata = euro.c[, -1]
head(mydata)
s = scale(mydata)
round(head(s),3)
class(s)
d = dist(mydata.s) #need a distance class in the Hierarchical Clustering portion
plot(mbc, what='uncertainty', dimens= c(6,8))
library(mclust)
mbc=Mclust(mydata)
table(mbc$classification)
plot(mbc,what='classification')
plot(mbc, what='uncertainty', dimens= c(6,8))
plot(mbc, what='uncertainty', dimens= c(6,8),pch="o")
plot(mbc, what='uncertainty', dimens= c(6,8),pch="15")
plot(mbc, what='uncertainty', dimens= c(6,8))
plot(mbc, what='uncertainty', dimens= c(6,8),cex=15)
plot(mbc, what='uncertainty', dimens= c(6,8))
data(banknote, package = "mclust")
mydata <- banknote[,-1]
# Scale your data.
bank <- banknote[,-1]
bank.s=scale(bank)
class(bank.s)
bank.d=as.dist(bank.s)
bank.d=dist(bank.s)
bank.d=dist(bank.s)
hc = hclust(bank.d,'single')
plot(hc, main = "Single Linkage HC Dendogram")
tbl=table(hc)
ct1=cutree(hc,2)
ct1
table(ct1)
table(ct1,banknote$Status)
hc2 = hclust(bank.d,'complete')
plot(hc2, main = "Complete Linkage HC Dendogram")
ct2=cutree(hc2,2)
table(ct2)
table(ct3,banknote$Status)
table(ct2,banknote$Status)
hc2 = hclust(bank.d,'complete')
plot(hc2, main = "Complete Linkage HC Dendogram")
ct2=cutree(hc2,2)
table(ct2)
table(ct2,banknote$Status)
hc3 = hclust(bank.d,'average')
plot(hc3, main = "Average Linkage HC Dendogram")
ct3=cutree(hc3,2)
table(ct3)
table(ct3,banknote$Status)
km = kmeans(bank.s,centers=2,nstart=10)
table(km,banknote$Status)
table(km$cluster,banknote$Status)
km$cluster
mbc = Mclust(bank,2)
table(mbc$classification,banknote$Status)
mbc = Mclust(bank,2)
table(mbc$classification,banknote$Status)
table(km$cluster,banknote$Status)
hc3 = hclust(bank.d,'average')
plot(hc3, main = "Average Linkage HC Dendogram")
ct3=cutree(hc3,2)
table(ct3)
table(ct3,banknote$Status)
hc2 = hclust(bank.d,'complete')
plot(hc2, main = "Complete Linkage HC Dendogram")
ct2=cutree(hc2,2)
table(ct2)
table(ct2,banknote$Status)
head(iris)
mydata1 = iris[,-5]
plot(mydata1, col = iris$Species)
mydata1.s = scale(mydata1)
hc1 = hclust(dist(mydata1.s), "single")
plot(rev(hc1$height)) # we have 3 clusters
ct1 = cutree(hc1, 3)
table(ct1)
table(ct1, iris$Species)
hc2 = hclust(dist(mydata1.s), "average")
plot(rev(hc2$height)) # Shows 4 groups, but we know that there are 3 groups in our data
ct2 = cutree(hc2, 3)
table(ct2)
table(ct2, iris$Species)
hc3 = hclust(dist(mydata1.s), "complete")
plot(rev(hc3$height)) # Shows 3 groups
ct3 = cutree(hc3, 3)
table(ct3)
table(ct3, iris$Species)
plot(mydata1, col = iris$Species, main = "colored based on True clusters")
plot(mydata1, col = ct3, main = "colored based on HC complete linkage")
plot(mydata1, col = ct2, main = "colored based on HC average linkage")
class(bank.s)
bank.d=dist(bank.s)
hc = hclust(bank.d,'single')
plot(hc, main = "Single Linkage HC Dendogram")
ct1=cutree(hc,2)
table(ct1)
table(ct1,banknote$Status)
table(bank)
table(mbc$classification,banknote$Status)
stock <- read.csv("http://tiny.cc/isqs6350_stockReturn")
# Multiplying by 100 to convert to % scale
mydata<-stock*100
cor(mydata)
install.packages('sem')
install.packages('semPlot')
library('sem')
evals.data <- factanal(data, factors = 3)
#Now we can code continent as binary
data.edited = data(,c(-1,-2))
#Read Libraries
library(readr)
library(readxl)
library(dplyr)
#Read data
population_tot <- read_csv("data/population_total.csv")
murder_total_deaths <- read_csv("data/murder_total_deaths edited.csv")
armed_forces_personnel_total <- read_csv("data/armed_forces_personnel_total.csv")
cell_phones <- read_csv("data/cell_phones_per_100_people.csv")
children_per_woman <- read_csv("data/children_per_woman_total_fertility.csv")
life_expectancy_years <- read_csv("data/life_expectancy_years.csv")
suicide_total_deaths <- read_csv("data/suicide_total_deaths.csv")
urban_population <- read_csv("data/urban_population.csv")
sex_ratio <- read_csv("data/sex_ratio_all_age_groups.csv")
corruption <- read_csv("data/corruption_perception_index_cpi.csv")
internet_users <- read_csv("data/internet_users.csv")
internet_users <- read_csv("data/internet_users.csv")
child_mortality <- read_csv("data/child_mortality_0_5_year_olds_dying_per_1000_born.csv")
income_per_person <- read_csv("data/income_per_person_gdppercapita_ppp_inflation_adjusted.csv")
investments_per_ofGDP <- read_csv("data/investments_percent_of_gdp.csv")
gini <- read_csv("data/gini.csv")
#####DATA#####
#POPULATION TOTAL
population_tot <- select(population_tot, "country", "2016")
colnames(population_tot) <- c("country", "pop_total")
#MURDER per person (from total)
murder_total_deaths <- select(murder_total_deaths, "Country", "2016")
colnames(murder_total_deaths) <- c("country", "murder_total")
murder_total_deaths <- merge(population_tot, murder_total_deaths , by="country", all = TRUE)
murder_total_deaths$divided <- murder_total_deaths$murder_total/murder_total_deaths$pop_total
murder_total_deaths <- select(murder_total_deaths,"country",'divided')
colnames(murder_total_deaths) <- c("country","murder_pp")
#ARMED per person (from total)
armed_forces_personnel_total <- select(armed_forces_personnel_total, "country", "2016")
colnames(armed_forces_personnel_total) <- c("country", "armed_total")
armed_forces_personnel_total <- merge(population_tot, armed_forces_personnel_total , by="country", all = TRUE)
armed_forces_personnel_total$divided <- armed_forces_personnel_total$armed_total/armed_forces_personnel_total$pop_total
armed_forces_personnel_total <- select(armed_forces_personnel_total,"country",'divided')
colnames(armed_forces_personnel_total) <- c("country","armed_pp")
#CELL PHONES per 100
cell_phones <- select(cell_phones, "country", "2016")
colnames(cell_phones) <- c("country", "phones_p100")
#URBAN POPULATION
urban_population<- select(urban_population, "country", "2016")
colnames(urban_population) <- c("country", "urban_pop_tot")
#CHILDREN PER WOMAN
children_per_woman <- select(children_per_woman, "country", "2016")
colnames(children_per_woman) <- c("country", "children_p_woman")
#LIFE EXPECTANCY
life_expectancy_years <- select(life_expectancy_years, "country", "2016")
colnames(life_expectancy_years) <- c("country", "life_exp_yrs")
#SUICIDE DEATHS TOTAL
suicide_total_deaths <- select(suicide_total_deaths, "country", "2016")
colnames(suicide_total_deaths) <- c("country", "suicide_total")
suicide_total_deaths <- merge(population_tot, suicide_total_deaths , by="country", all = TRUE)
suicide_total_deaths$divided <- suicide_total_deaths$suicide_total/suicide_total_deaths$pop_total
suicide_total_deaths <- select(suicide_total_deaths,"country",'divided')
colnames(suicide_total_deaths) <- c("country","suicide_pp")
#CORRUPTION
corruption <- select(corruption, "country", "2016")
colnames(corruption) <- c("country", "corruption_CPI")
#INTERNET USERS (%of pop)
internet_users <- select(internet_users, "country", "2016")
colnames(internet_users) <- c("country", "internet_%of_pop")
#SEX RATIO IT IS 2015!!!!!! #man/women per 100
sex_ratio <- select(sex_ratio, "country", "2015")
colnames(sex_ratio) <- c("country", "sex_ratio_p100")
#CHILDREN MORTALITY
# 0-5 year olds dying per 1000 born
child_mortality <- select(child_mortality, "country", "2016")
colnames(child_mortality) <- c("country", "child_mort_p1000")
#INCOME PER PERSON
income_per_person <- select(income_per_person, "country", "2016")
colnames(income_per_person) <- c("country", "income_per_person")
#INVESTMENT AS A % OF DGP
investments_per_ofGDP <- select(investments_per_ofGDP, "country", "2016")
colnames(investments_per_ofGDP) <- c("country", "investments_per_ofGDP")
#GINI
gini <- select(gini, "country", "2016")
colnames(gini) <- c("country", "gini")
#Merge all
data <- merge(population_tot, murder_total_deaths, by="country", all = TRUE)
data <- merge(data, armed_forces_personnel_total , by="country", all = TRUE)
data <- merge(data, cell_phones , by="country", all = TRUE)
data <- merge(data, children_per_woman , by="country", all = TRUE)
data <- merge(data, life_expectancy_years , by="country", all = TRUE)
data <- merge(data, suicide_total_deaths , by="country", all = TRUE)
data <- merge(data, urban_population , by="country", all = TRUE)
data <- merge(data, sex_ratio , by="country", all = TRUE)
data <- merge(data, corruption , by="country", all = TRUE)
data <- merge(data, internet_users , by="country", all = TRUE)
data <- merge(data, child_mortality, by="country", all = TRUE)
data <- merge(data, income_per_person, by="country", all = TRUE)
data <- merge(data, investments_per_ofGDP, by= "country", all=TRUE)
data <- merge(data, gini, by="country", all=TRUE)
########################
######NA'S
#Check numbers of NA's
summary(data)
###### CORRELATIONS
# simple matrix correlation plot
plot(data[,-1])
# more fency correlation matrix plot
pairs(data[,-1],
panel = function (x, y, ...) {
points(x, y, ...)
abline(lm(y ~ x), col = "red")
}, pch = ".", cex = 1.5)
# super fency correlation
#install.packages("PerformanceAnalytics")
library("PerformanceAnalytics")
chart.Correlation(data[,-1], histogram=TRUE, pch=42)
# correlation matrix (numbers)
cor(data[,-1], use = "complete.obs")
cor(data[,-1], use = "pairwise.complete.obs")
#Check PC
data_test <- na.omit(data)
data
a <- princomp(data_test[,-1], cor=T)
summary(a, loadings=T)
# HEre is a test where I deleted few columns.
# See how PC increases/descreases while deleting less correlated columns
## TEST PCA WITHOUT FEW COLUMNS
test <- data[,-c(2,4,9,10,15)] ## optioaly delete murder_pp
test2 <- na.omit(test)
b <- princomp(test2[,-1], cor=T)
summary(b, loadings=T)
chart.Correlation(test[,-1], histogram=TRUE, pch=42)
# Create new a column continent and divide countires by continent
library(countrycode)
data$continent <- countrycode(sourcevar = data[, "country"],
origin = "country.name",
destination = "continent")
# Just rearanage columns to get continent before country
data <- data[,c(ncol(data),seq(1,ncol(data)-1,by=1))]
#The function above did not divide coutries into America North, Central, South so I did this by hand
subset(data, continent=='Americas') %>% select(country) %>% as.list()
America_North <- c("Canada","United States","Mexico")
America_South <- c("Peru","Ecuador","Chile","Bolivia","Venezuela","Paraguay","Brazil",
"Colombia",  "Guyana", "Suriname", "Uruguay", "Argentina")
#Change the values from "Americas" into more specific part of America
for (i in 1:nrow(data)){
ifelse(data$continent[i] == "Americas",
ifelse(data$country[i] %in% America_North,data$continent[i] <- 'North America',
ifelse(data$country[i] %in% America_South,data$continent[i] <- 'South America',
data$continent[i] <- 'Central America')), data$country[i])
}
#Now we can code continent as binary
data.edited = data(,c(-1,-2))
evals.data <- factanal(data, factors = 3)
#Now we can code continent as binary
data.edited = data[,c(-1,-2)]
evals.data <- factanal(data.edited, factors = 3)
data.edited
#Now we can code continent as binary
data.edited = na.omit(data[,c(-1,-2)])
evals.data <- factanal(data.edited, factors = 3)
evals.data
evals.data <- factanal(data.edited, factors = 7)
evals.data
evals.data <- factanal(data.edited, factors = 6)
evals.data
evals.data <- factanal(data.edited, factors = 6)
#Now we can code continent as binary
data.edited = na.omit(data[,c(-1,-2)])
evals.data <- factanal(data.edited, factors = 6)
evals.data
evals.data <- factanal(data.edited, factors = 5)
evals.data
evals.data <- factanal(data.edited, factors = 4)
evals.data
evals.data <- factanal(data.edited, factors = 6)
evals.data <- factanal(data.edited, factors = 8)
evals.data
evals.data <- factanal(data.edited, factors = 9)
evals.data <- factanal(data.edited, factors = 7)
evals.data
evals.data$loadings
evals.data <- factanal(data.edited, factors = 7)
evals.data <- factanal(data.edited, factors = 3)  #P-value = 0.506
evals.data
evals.data <- factanal(data.edited, factors = 4)  #P-value = 0.506
evals.data
evals.data <- factanal(data.edited, factors = 5)  #P-value = 0.506
evals.data
evals.data <- factanal(data.edited, factors = 6)  #P-value = 0.506
evals.data
evals.data <- factanal(data.edited, factors = 2)  #P-value = 0.506
evals.data
evals.data <- factanal(data.edited, factors = 7)  #P-value = 0.506
test <- read.csv("http://tiny.cc/isqs6350_test", header = T)
head(test)
cov(test)
cor(test) # correlation is the standardized form of covariance.
test.pca <- princomp(test, cor = T) # Why "cor = T"? To find principal components of the standardized data.
summary(test.pca, loading = T) #loading = T is the same as the eigenvector, so we need to see them and loading = T shows us them
legen
?legend
